# 6.867

Notes collected from office hours:
* There is no clear convergence criterion. Common methods include norm of the gradient, finite difference (small changes in objective cause indicate convergence), and a fixed number of iterations.
* There is no rule of thumb for choosing learning rate / step size.
* There is no clear way to make stochastic gradient descent converge. Typically, people use a fixed number of iterations.

